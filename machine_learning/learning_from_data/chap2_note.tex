\chapter{Training Versus Testing}

\section{Training versus Testing}
\subsection{Theory of Generalization}
\begin{itemize}[noitemsep, topsep=0pt]
    \item The out-of-sample error $E_{out}$ measures how well our training on \textit{D} has generalized to data that we have not seen before. It is based on the performance over the entire input space \textit{X}. If we want to estimate it, we should use unseen ``fresh'' data points.
    \item The in sample error $E_{in}$, by contrast, is based on data points that have been used for training.
    \item \textbf{\textit{Generalization error}}: the discrepancy between $E_{in}$ and $E_{out}$. Notice: sometime, `generalization error' is also used for another name of $E_{out}$, but not for this text. We have already provides a way to characterize the generalization error with a probabilistic bound in chapter 1,
        \begin{equation}
            \mathbb{P}[|E_{in}(g) - E_{out}(g)| > \epsilon] \leq 2Me^{-2e^2N},
        \end{equation}
        for any $\epsilon > 0$
    \item The generalization error mentioned above can be rephrased as follows.
        Pick a tolerance level $\delta$, for example, $\delta = 0.5$,
        we assert with probability at least $1 - \delta$.
        Then, we can get $\bb{P}[|E_{in}(g) - E_{out}(g)| > \epsilon] \le 2Me^{-2e^2N} \le \delta$.\\
        Finally we can reach
        \begin{equation} \label{Training Versus Testing: generalization bound}
            E_{out}(g) \le E_{in}(g) + \sqrt{\frac{1}{2N} ln \frac{2M}{\delta}}.
        \end{equation}
        We refer to the type of inequality in the form \ref{Training Versus Testing: generalization bound}
        as a \bf{\it{generalization bound}} because it bounds $E_{out}$ in terms of $E_{in}$
    \item (accuracy upper bound) It is worth noticing that the other side of $|E_{out} - E_{in}| \le \epsilon$ also holds,
        that is, $E_{out} \ge E_{in} - \epsilon$.
        This is important for learning. 
        The $E_{out} \ge E_{in} - \epsilon$ direction of the bound assures us that we couldn't do significantly better
        if we have a relatively high $E_{in}$.
    \item (limitation) The error bound $\sqrt{\frac{1}{2N} ln \frac{2M}{\delta}}$ in \ref{Training Versus Testing: generalization bound}
        depends on $M$, the size of the hypothesis set $\mathcal{H}$. 
        If $\mathcal{H}$ is a infinite set,
        the bound will goes to infinity,
        thus the bound becomes meaningless.
        Unfortunately, almost all interesting hypothesis set $\mathcal{H}$ are infinite.
    \item (The cause of the limitation) We over-estimate the the probability using union bound. 
        Let's denote $\mathcal{B}_m$ be ($\mathcal{B}$ad) event that ``$|E_{in}(h_m) - E_{out}(h_m)| > \epsilon$'',
        Then our assumption
        \begin{equation}
            \bb{P}[\mathcal{B}_1 \bf{ or } \mathcal{B}_2 \bf{ or } \cdots \bf{ or }\mathcal{B}_M]
            \le \bb{P}[\mathcal{B}_1] + \bb{P}[\mathcal{B}_2] + \cdots + \bb{P}[\mathcal{B}_M].
        \end{equation}
        The events $\bb{B}_1, \cdots, \bb{B}_M$ are more likely strongly overlapping,
        so our estimate is bad enough in this case.
\end{itemize}

\subsection{Effective Number of Hypothesis}
To overcome the limitation mentioned above, we introduce \it{growth function},
the quantity that will formalize the effective number of hypothesis.
The growth function is what will replace $M$ in the generalization bound \ref{Training Versus Testing: generalization bound}.
It is a combinatorics quantity that captures how different the hypothesis in $\mathcal{H}$ are,
and hence how much overlap the different events have.\par
We will focus on binary target function for the purpose of this analysis.\par

First we have to introduce hichotomies.
\begin{mydef}
    Let $\bf{x}_1, \cdots, \bf{x}_N \in \mathcal{X}$. The dichotomies generated by $\mathcal{H}$ on these points are defined by
    \begin{equation}
        \mathcal{H}(\bf{x}_1, \cdots, \bf{x}_N) = \{ (h(\bf{x}_1), \cdots, h(\bf{x}_N)) | h \in \mathcal{H} \}.
    \end{equation}
\end{mydef}

Based on hichotomies, we have our definition of growth function.
\begin{mydef}
    The growth function is defined for a hypothesis set $\mathcal{H}$ by 
    \begin{equation}
        m_{\mathcal{H}}(N) = \max_{\bf{x}_1, \cdots, \bf{x}_N \in \mathcal{X}} |\mathcal{H}(\bf{x}_1, \cdots, \bf{x}_N)|,
    \end{equation}
    where $|\cdot|$ denote the cardinality of a set.
\end{mydef}

By this way, we bound the size of the growth function by choosing $N$ point from $\mathcal{X}$. 
which is 
\begin{equation}
    m_{\mathcal{H}}(N) \le 2^N.
\end{equation}

If $\mathcal{H}$ is capable of generating all possible dichotomies on $\bf{x}_1, \cdots, \bf{x}_N$,
meaning that $\mathcal{H}(\bf{x}_1, \cdots, \bf{x}_N) = \{-1, +1\}^N$ 
and we say that $\mathcal{H}$ can \bf{\it{shatter}} $\bf{x}_1, \cdots, \bf{x}_N$







